---
title: "M4_W4_L1_Case_Studies"
author: "Alexander Cormack"
date: "2022-12-11"
output: html_document
---


# Case Studies

## Samsung human activity case study

Data is sourced from the University of California Irvine Machine Learning Repository.

### Slightly processed data

For this lecture we will just use the training dataset ... here is some code to prepare the dataframe

```{r}
library(dplyr)

feature_names <- read.table("./data/UCI_HAR_Dataset/features.txt")
features <- feature_names$V2

subject_train <- read.table("./data/UCI_HAR_Dataset/train/subject_train.txt")
colnames(subject_train) <- "subject"

activity_train <- read.table("./data/UCI_HAR_Dataset/train/y_train.txt")
colnames(activity_train) <- "activity"

activity_train$activity[activity_train$activity == 1] <- "walk"
activity_train$activity[activity_train$activity == 2] <- "walkup"
activity_train$activity[activity_train$activity == 3] <- "walkdown"
activity_train$activity[activity_train$activity == 4] <- "sitting"
activity_train$activity[activity_train$activity == 5] <- "standing"
activity_train$activity[activity_train$activity == 6] <- "laying"

train_measurements <- read.table("./data/UCI_HAR_Dataset/train/X_train.txt")
colnames(train_measurements) <- features

samsungData <- bind_cols(train_measurements, subject_train, activity_train) 
```

```{r}
names(samsungData)[1:12]
```
```{r}
table(samsungData$activity)
```

### Plotting average accelaration for first subject

```{r}
par(mfrow = c(1, 2), mar = c(5, 4, 1, 1))
samsungData <- transform(samsungData, activity = factor(activity))
sub1 <- subset(samsungData, subject == 1)
plot(sub1[, 1], col = sub1$activity, ylab = names(sub1)[1])
plot(sub1[, 2], col = sub1$activity, ylab = names(sub1)[2])
legend("bottomright", legend = unique(sub1$activity), col = unique(sub1$activity), pch = 1)
```

### Clustering based just on average acceleration

The cluster dendogram of average acceleration is not very informative ... all the colours are jumbled together so there is no clear differentiation between the different activities.

```{r}
source("myplclust.R")
distanceMatrix <- dist(sub1[, 1:3])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))
```

### Plotting maximum acceleration for the first subject

Let's instead look at maximum acceleration for the first subject in the x and y directions (columns 10 & 11).

We can see that there is little variability for the passive activities but much more variability for the active activities. So maximum acceleration may be a predictor for the active activities or at least for separating the passive from the active activities.

```{r}
par(mfrow = c(1, 2))
plot(sub1[, 10], pch = 19, col = sub1$activity, ylab = names(sub1[10]))
plot(sub1[, 11], pch = 19, col = sub1$activity, ylab = names(sub1[11]))
```

### Clustering data based on maximum acceleration

Here we can see a clear separation between the passive and the active activities. Walkdown is clearly distinct but there is some mixing between walk and walkup and the passive activities are still quite jumbled

```{r}
source("myplclust.R")
distanceMatrix <- dist(sub1[, 10:12])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))
legend("topright", legend = unique(sub1$activity), col = unique(sub1$activity), pch = 1)
```

### Singular value decomposition

We remove the subject and the activity columns and perform singular value decomposition. The first singular value separates the values quite well, while the separation is a little more vague with the second singular value ... walkup is perhaps the most clearly separated activity.

```{r}
svd1 <- svd(scale(sub1[, -c(562:563)]))
par(mfrow = c(1, 2))
plot(svd1$u[, 1], col = sub1$activity, pch = 19)
legend("bottomright", legend = unique(sub1$activity), col = unique(sub1$activity), pch = 1)
plot(svd1$u[, 2], col = sub1$activity, pch = 19)
```

### Find the maximum contributor

Let's now try to find the feature that contributes the most to the variation in the data. Here we are looking at the second right singular vector

```{r}
plot(svd1$v[, 2], pch = 19)
```

### New clustering with maximum contributor

We can use the which.max() function to determine the feature that is the maximum contributor. The three active activities are now much more separated, but it didn't seem to help a lot with the passive activities.

```{r}
maxContrib <- which.max(svd1$v[, 2])
distanceMatrix <- dist(sub1[, c(10:12, maxContrib)])
hclustering <- hclust(distanceMatrix)
myplclust(hclustering, lab.col = unclass(sub1$activity))
```

So what is the maximum contributor? It is the mean body accelaration in the frequency domain in the z direction:

```{r}
names(samsungData[maxContrib])
```

### K-means clustering (nstart=1, first try)

```{r}
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6)
table(kClust$cluster, sub1$activity)
```

### K-means clustering (nstart = 1, second try)

```{r}
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 1)
table(kClust$cluster, sub1$activity)
```

### K-means clustering (nstart = 100, first try)

```{r}
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 100)
table(kClust$cluster, sub1$activity)
```

### K-means clustering (nstart = 100, second try)

```{r}
kClust <- kmeans(sub1[, -c(562, 563)], centers = 6, nstart = 100)
table(kClust$cluster, sub1$activity)
```

### Cluster 1 variable centers (Laying)

By looking at the cluster centers we can understand what are the features that drive the location of the cluster center. Here looking at laying we can see that the average acceleration in the three dimensions are the most significant. (Disclaimer - in the knitted file the graph may not show laying, since the centers are randomly allocated by kmeans and the laying center may not be the fifth row as it is in the .Rmd file)


```{r}
plot(kClust$center[5, 1:10], pch = 19, ylab = "Cluster Center", xlab = "")
```

### Cluster 2 variable centers (Walking)

Instead when we look at walking we can see that there are other components that have an influence on where the center lies. This can give you a hint as to which features will be most interesting for predicting that activity.  (Disclaimer - in the knitted file the graph may not show walking, since the centers are randomly allocated by kmeans and the laying center may not be the second row as it is in the .Rmd file)

```{r}
plot(kClust$center[2, 1:10], pch = 19, ylab = "Cluster Center", xlab ="")
```


## Air pollution case study

We are going to be comparing air pollution data from 1999 and 2012. I have downloaded the daily pm2.5 measurements for 1999 and 2012 from the EPA website (https://aqs.epa.gov/aqsweb/airdata/download_files.html) and stored them in the data folder. 

Our basic question is: **How has air pollution changed in the period 1999-2012.**

Let's read in the 1999 file and see what it contains.

```{r}
pm0 <- read.csv("./data/daily_88101_1999.csv")
str(pm0)
```

Let's take a look at the pm2.5 measurements

```{r}
x0 <- pm0$Arithmetic.Mean
class(x0)
```
```{r}
str(x0)
```
```{r}
summary(x0)
```

Let's now read in the 2012 data and do the same things

```{r}
pm1 <- read.csv("./data/daily_88101_2012.csv")
str(pm0)
```
```{r}
x1 <- pm1$Arithmetic.Mean
class(x1)
```
```{r}
summary(x1)
```
Let's just compare the summary to the 1999 data
```{r}
summary(x0)
```

We can quickly see that there is quite a difference between the mean and median values of the two data sets.

Now let's plot the two measurements

```{r}
boxplot(x0, x1)
```

We can see that the data is significantly skewed to the right ... as we saw there the maximum values are very high.

So to fix the outliers we can take the log of the data.
```{r}
boxplot(log10(x0), log10(x1))
```

We can see that the average value has gone down in 2012, but there is also a greater amount of spread in the data.

We saw in the 2012 summary data that there are negative values ... that's a bit strange given that the measurement is a mass. So let's take a closer look at the negative values.

```{r}
negative <- x1 < 0
str(negative)
```

If we take the sum of this logical vector we will get the number of negative values.

```{r}
sum(negative, na.rm = TRUE)
```
```{r}
mean(negative, na.rm = TRUE)
```

So there are 1130 negative values and they make up about half a percent of all of the values. That's not a great amount so we can perhaps ignore them. Still we might like to see if there is some pattern to these negative values. Maybe they occur on particular days? Let's take a look then at the dates of these values.

```{r}
dates <- pm1$Date.Local
str(dates)
```

We can see that they are character strings, so let's convert them to date format.

```{r}
dates <- as.Date(dates, "%d/%m/%Y")
str(dates)
```


Let's now take a histogram of the dates by month.

```{r}
hist(dates, "month")
```

Now let's see where the negative values are.

```{r}
hist(dates[negative], "month")
```

pm2.5 tends to be more prevalent in the summer months than in the winter months, so it may be more difficult to measure in the winter months and hence lead to measurement errors. This would explain the spike in December, but not the peaks in June and July. So perhaps we don't need to worry too much about this since they make up such a small portion of the data.


Let's now compare values between the two data sets but this time from a single monitor. This can help us control for possible changes in the monitoring locations between 1999 and 2012. Let's pick a monitor in NY State.

Let's take a subset of all of the monitors in NY State (code =36) in both 1999 and 2012 and let's also get the County Code and the Site ID.

```{r}
site0 <- unique(subset(pm0, State.Code == 36, c(County.Code, Site.Num)))
site1 <- unique(subset(pm1, State.Code == 36, c(County.Code, Site.Num)))
```

This gives us a 2-column dataframe with the County.Code and the Site.Num.

```{r}
head(site0)
```

Let's now make a special code by pasting those two values together.

```{r}
site0 <- paste(site0[, 1], site0[, 2], sep = ".")
site1 <- paste(site1[, 1], site1[, 2], sep = ".")
```
```{r}
str(site0)
```
```{r}
str(site1)
```

We can quickly see that there are 33 monitors in 1999 and only 18 in 2012. We are interested in the intersection of these two vectors, i.e. the monitors that are present in both 1999 and 2012.

```{r}
both <- intersect(site0, site1)
both
```

We see that there are 11 monitors that are common to both vectors ... so we have a good number to look at to compare the data across the years.

It would also be useful to look at those monitors that have a lot of observations ... so let's see how many observations each of the monitors has.

So now let's add a new variable to the original dataframes for the two years - a column with out county.site code. Then we want to subset the dataframe for only the monitors in NY State.

```{r}
pm0$County.Site <- with(pm0, paste(County.Code, Site.Num, sep = "."))
pm1$County.Site <- with(pm1, paste(County.Code, Site.Num, sep = "."))
cnt0 <- subset(pm0, State.Code == 36 & County.Site %in% both)
cnt1 <- subset(pm1, State.Code == 36 & County.Site %in% both)
head(cnt0)
```

Now we want to split this dataframe to separate out the different monitors so we can then see how many observations there are for each monitor. So let's use the function sapply to count the number of rows in each of the dataframes we would created by splitting them.

```{r}
sapply(split(cnt0, cnt0$County.Site), nrow)
```
```{r}
sapply(split(cnt1, cnt1$County.Site), nrow)
```

So comparing the observations across the years we might choose County 101 and monitor 3 as there are 108 observations in 1999 and 112 observations in 2012.

Let's now make two new dataframes - subsets of the County.Site 101.3

```{r}
pm0sub <- subset(pm0, State.Code == 36 & County.Code == 101 & Site.Num == 3)
pm1sub <- subset(pm1, State.Code == 36 & County.Code == 101 & Site.Num == 3)
```

```{r}
dim(pm0sub)
```
```{r}
dim(pm1sub)
```

Now we are going to plot the pm2.5 of these two dataframes to see the changes over time. So first of all we need to get the dates and the pm2.5 data out of the dataframes.

```{r}
dates0 <- pm0sub$Date.Local
dates0 <- as.Date(dates0, "%d/%m/%Y")
x0sub <- pm0sub$Arithmetic.Mean
plot(dates0, x0sub)
```
```{r}
dates1 <- pm1sub$Date.Local
dates1 <- as.Date(dates1, "%d/%m/%Y")
x1sub <- pm1sub$Arithmetic.Mean
plot(dates1, x1sub)
```

It's a little hard to look at them separately, so let's put them in a panel.

```{r}
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
plot(dates0, x0sub, pch = 19)
abline(h = median(x0sub, na.rm = TRUE))
plot(dates1, x1sub, pch = 19)
abline(h = median(x1sub, na.rm = TRUE))
```

As the Y axes have different values the plot is a little misleading ... so we need to put the individual plots on the same range.

Let's calcuate the range of the two datasets using the range function.

```{r}
range(x0sub, x1sub, na.rm = TRUE)
```

Now let's assign the range to a variable and set the range to the plots.

```{r}
rng <- range(x0sub, x1sub, na.rm = TRUE)
par(mfrow = c(1, 2))
plot(dates0, x0sub, pch = 20, ylim = rng)
abline(h = median(x0sub, na.rm = TRUE))
plot(dates1, x1sub, pch = 20, ylim = rng)
abline(h = median(x1sub, na.rm = TRUE))
```

So we can see that the average has gone down over the years, and there is also a reduction in the spikes of data.

Let's now compare the measurements at the state level (useful since the States are responsible for compliance). So we want to take the average for each of the states in both 1999 and 2012 and connect them in a plot to see if they have gone up, down or remained the same.

This is the perfect case for using the tapply() function.

```{r}
mn0 <- with(pm0, tapply(Arithmetic.Mean, State.Code, mean, na.rm = TRUE))
mn1 <- with(pm1, tapply(Arithmetic.Mean, State.Code, mean, na.rm = TRUE))
d0 <- data.frame(state = names(mn0), mean = mn0)
d1 <- data.frame(state = names(mn1), mean = mn1)
mrg <- merge(d0, d1, by = "state")
head(mrg)
```

So now we have a dataframe with one row for each state and the mean pm2.5 values for 1999 and 2012. So let's plot them now and connect them with a line.

```{r}
par(mfrow = c(1, 1))
with(mrg, plot(rep(1999, 51), mrg[, 2], xlim = c(1998, 2013)))
with(mrg, points(rep(2012, 51), mrg[, 3]))
segments(rep(1999, 51), mrg[, 2], rep(2012, 51), mrg[, 3])
```

So we can see the trend is for the pollution levels to be going down in most states, although there are some where it has changed very little or actually gone up.
