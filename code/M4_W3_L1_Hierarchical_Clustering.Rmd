---
title: "M4_W3_L1_Hierarchical_Clustering"
author: "Alexander Cormack"
date: "2022-11-26"
output: html_document
---

# Hierarchical Clustering

## Can we find things that are close together?

Clustering organises things that are close into groups

- How do we define close?
- How do we group things?
- How do we visualize the grouping?
- How do we interpret the grouping?


## Hierarchical clustering

An agglomerative approach

- find closest two things
- put them together
- find next closest

This requires:

- a defined distance
- a merging approach

This produces:

- a tree showing how close things are to each other


## How do we define close?

This is the most important step. If we don't define what close means, we will end up with garbage in -> garbage out

Distance or similarity

- Continuous - Eucliedean distance
- Continuous - correlation similarity
- Binary - Manhattan distance

Pick a distance / similarity that makes sense for your problem


Hierarchical clustering - example

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, labels = as.character(1:12))
```

## Hierarchical clustering - dist

Important parameters: *x, method*

```{r}
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame)
```

The output above shows the distance between each of the points. If we were to start clustering, the first two points to be clustered would be points 5 & 6 as these are the closest two points (0.081). The next two points to be clustered would be 10 & 11 (0.083).


## Hierarchical clustering - hclust

Here we perform the clustering on the data points and print out the cluster dendogram.

```{r}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
```

We can see for example that points 5 & 6 were clustered together, and then they were clustered with the next nearest point (7) and then again with the next nearest point (8) and so on.

What the dendogram DOESN'T tell us is how many clusters there are. So we would need to cut the tree at a certain point along its height and see how many branches there are. For example, if we cut the tree at 2.0 (by drawing a horizontal line across the graph) we would run into two branches, i.e. two clusters. However, if we cut the tree at 1.0 we would run into three branches, and so three clusters.

So we would need to cut the tree at a convenient point, but there is no rule to say where that convenient point is.


## Prettier dendograms

Check out: http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79


## Merging points

When you merge two points together there are several ways for determining what the new location of the clustered points is.

One is called average linkage - the new location is simply the average of their x coordinates and y coordinates, i.e. the centre of gravity of those two points or clusters.

Another is called complete linkage - here the distance between two clusters of points is the distance between the two farthest points in the two clusters.

There is no right or wrong approach, although they can give different results, so it is often useful to try both to see the results you get.


## heatmap()

This is a really nice function for visualising matrix data. It runs a hierarchical analysis on the rows of a table and on the columns of a table. The function creates an image plot and reorders the columns and rows of the table according to the hierarchical clustering algorithm. 

```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
heatmap(dataMatrix)
```

## Notes and further resources

Hierarchical clustering is a useful technique that gives an idea of the relationship between variables/observations

The picture may be unstable

- change a few points
- have different missing values
- pick a different distance
- change the merging strategy
- change the scale of points for one variable

However, hierarchical clustering is deterministic

Choosing where to cut isn't always obvious

Hierarchical clustering should really be used primarily for exploration










