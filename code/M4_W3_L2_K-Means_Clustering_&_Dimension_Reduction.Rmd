---
title: "M4_W3_L2_K-Means_Clustering_&_Dimension_Reduction"
author: "Alexander Cormack"
date: "2022-11-30"
output: html_document
---


# K-Means Clustering


## Can we find things that are close together?

How do we define close?

How do we group things?

How do we visualise grouping?

How do we interpret the grouping?


### How do we define close?

Most important step:

- gargage in -> garbage out

Distance or similarity?

- Continuous - Euclidean distance

- Continuous - correlation similarity

- Binary - Manhattan distance

Pick a distance/similarity that makes sense for your problem


### K-means clustering

A partitioning approach

- Fix a number of clusters

- Get "centroids" of each cluster

- Assign things to closest centroid

- Recalculate centroid


This requires:

- A defined metric distance

- A number of clusters

- An initial guess as to cluster centroids


This produces:

- Final estimate of cluster centroids

- An assignment of each point to clusters


### K-means clustering - example

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```

The process of k-means is to assign the centroid to the data (we would choose three centroids as it looks like there are three clusters in the data above).

Points are then assigned to the centroids, the centre of the clusters are calculated and  the centroids are shifted accordingly.

The points are then re-assigned to the clusters and the centroids recalculated in an interative process until stable clusters are formed.


### kmeans()

Important parameters: *x, centers, iter.max, nstart*

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
```
```{r}
kmeansObj$cluster
```

Plotting the result of kmeans()

```{r}
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, lwd =3)
```

### Heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```


### Notes and further resources

KI-means requires a number of clusters

- pick by eye / intuition
- pick by cross validation / information theory, etc.
- determining the number of clusters

K-means is not deterministic

- different number of clusters

- different number of iterations


Rafael Irizarry's Distances and Clustering video

Elements of statistical learning










