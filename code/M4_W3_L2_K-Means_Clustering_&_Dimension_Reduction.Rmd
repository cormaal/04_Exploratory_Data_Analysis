---
title: "M4_W3_L2_K-Means_Clustering_&_Dimension_Reduction"
author: "Alexander Cormack"
date: "2022-11-30"
output: html_document
---


# K-Means Clustering


## Can we find things that are close together?

How do we define close?

How do we group things?

How do we visualise grouping?

How do we interpret the grouping?


### How do we define close?

Most important step:

- gargage in -> garbage out

Distance or similarity?

- Continuous - Euclidean distance

- Continuous - correlation similarity

- Binary - Manhattan distance

Pick a distance/similarity that makes sense for your problem


### K-means clustering

A partitioning approach

- Fix a number of clusters

- Get "centroids" of each cluster

- Assign things to closest centroid

- Recalculate centroid


This requires:

- A defined metric distance

- A number of clusters

- An initial guess as to cluster centroids


This produces:

- Final estimate of cluster centroids

- An assignment of each point to clusters


### K-means clustering - example

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```

The process of k-means is to assign the centroid to the data (we would choose three centroids as it looks like there are three clusters in the data above).

Points are then assigned to the centroids, the centre of the clusters are calculated and  the centroids are shifted accordingly.

The points are then re-assigned to the clusters and the centroids recalculated in an interative process until stable clusters are formed.


### kmeans()

Important parameters: *x, centers, iter.max, nstart*

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
```
```{r}
kmeansObj$cluster
```

Plotting the result of kmeans()

```{r}
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, lwd =3)
```

### Heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```


### Notes and further resources

KI-means requires a number of clusters

- pick by eye / intuition
- pick by cross validation / information theory, etc.
- determining the number of clusters

K-means is not deterministic

- different number of clusters

- different number of iterations


Rafael Irizarry's Distances and Clustering video

Elements of statistical learning


# Dimension Reduction

## Principal components analysis and singular value decomposition

### Matrix data

Here we generate some random normal data and plot it using the image function.

```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

It looks pretty noisy and there is no real pattern.

We could run a hierarchical cluster analysis and use the heatmap function:

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

We can see that the cluster analysis is done, the dendograms are printed on both the columns and the rows, but still no real interesting pattern emerges because there is no real interesting pattern in the underlying data.

So let's add a pattern to the data. We can do this by looping through the data and adding a pattern to a row based on the flip of a coin:

```{r}
set.seed(678910)
for (i in 1:40) {
        # flip a coin
        coinFlip <- rbinom(1, size = 1, prob = 0.5)
        # if coin is heads add a common pattern to that row
        if (coinFlip) {
                dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
        }
}
```
```{r}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

Some of the rows have a mean of 3 on the right-hand side and some of them don't, so we can see some kind of a pattern.

Let's now run the hierarchical cluster analysis

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

We can clearly see that the columns are split into two clusters with five on the left and five on the right. As there is no pattern in the rows the dendogram there is rather messy.


### Patterns in rows and columns

We can take a closer look at the patterns in the rows and the columns by looking at the marginal means of the rows and columns. Here we use some code to do just that.

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)
```

### Related problems

You have multivariate variable X1 ..., Xn so X1 = (X11, ..., X1m)

- Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible
- If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

The first goal is statistical and the second goal is data compression


### Related solutions - PCA / SVD

**SVD**

If *X* is a matrix with each variable in a column and each observation in a row then the singular value decomposition is a "matrix decomposition"

*X = UDV^T*

where teh columns of *U* are orthogonal (left singular vectors), the columns of *V* are orthogonal (right singular vectors) and *D* is a diagonal matrix (singular values)


**PCA**

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables
























