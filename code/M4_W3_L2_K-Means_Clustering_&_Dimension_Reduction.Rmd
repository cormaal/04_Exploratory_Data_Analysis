---
title: "M4_W3_L2_K-Means_Clustering_&_Dimension_Reduction"
author: "Alexander Cormack"
date: "2022-11-30"
output: html_document
---


# K-Means Clustering


## Can we find things that are close together?

How do we define close?

How do we group things?

How do we visualise grouping?

How do we interpret the grouping?


### How do we define close?

Most important step:

- gargage in -> garbage out

Distance or similarity?

- Continuous - Euclidean distance

- Continuous - correlation similarity

- Binary - Manhattan distance

Pick a distance/similarity that makes sense for your problem


### K-means clustering

A partitioning approach

- Fix a number of clusters

- Get "centroids" of each cluster

- Assign things to closest centroid

- Recalculate centroid


This requires:

- A defined metric distance

- A number of clusters

- An initial guess as to cluster centroids


This produces:

- Final estimate of cluster centroids

- An assignment of each point to clusters


### K-means clustering - example

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```

The process of k-means is to assign the centroid to the data (we would choose three centroids as it looks like there are three clusters in the data above).

Points are then assigned to the centroids, the centre of the clusters are calculated and  the centroids are shifted accordingly.

The points are then re-assigned to the clusters and the centroids recalculated in an interative process until stable clusters are formed.


### kmeans()

Important parameters: *x, centers, iter.max, nstart*

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)
```
```{r}
kmeansObj$cluster
```

Plotting the result of kmeans()

```{r}
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3, lwd =3)
```

### Heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1, 2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```


### Notes and further resources

KI-means requires a number of clusters

- pick by eye / intuition
- pick by cross validation / information theory, etc.
- determining the number of clusters

K-means is not deterministic

- different number of clusters

- different number of iterations


Rafael Irizarry's Distances and Clustering video

Elements of statistical learning


# Dimension Reduction

## Principal components analysis and singular value decomposition

### Matrix data

Here we generate some random normal data and plot it using the image function.

```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

It looks pretty noisy and there is no real pattern.

We could run a hierarchical cluster analysis and use the heatmap function:

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

We can see that the cluster analysis is done, the dendograms are printed on both the columns and the rows, but still no real interesting pattern emerges because there is no real interesting pattern in the underlying data.

So let's add a pattern to the data. We can do this by looping through the data and adding a pattern to a row based on the flip of a coin:

```{r}
set.seed(678910)
for (i in 1:40) {
        # flip a coin
        coinFlip <- rbinom(1, size = 1, prob = 0.5)
        # if coin is heads add a common pattern to that row
        if (coinFlip) {
                dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 3), each = 5)
        }
}
```
```{r}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

Some of the rows have a mean of 3 on the right-hand side and some of them don't, so we can see some kind of a pattern.

Let's now run the hierarchical cluster analysis

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

We can clearly see that the columns are split into two clusters with five on the left and five on the right. As there is no pattern in the rows the dendogram there is rather messy.


### Patterns in rows and columns

We can take a closer look at the patterns in the rows and the columns by looking at the marginal means of the rows and columns. Here we use some code to do just that.

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", ylab = "Column Mean", pch = 19)
```

### Related problems

You have multivariate variable X1 ..., Xn so X1 = (X11, ..., X1m)

- Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible
- If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

The first goal is statistical and the second goal is data compression


### Related solutions - PCA / SVD

**SVD**

If *X* is a matrix with each variable in a column and each observation in a row then the singular value decomposition is a "matrix decomposition"

*X = UDV^T*

where teh columns of *U* are orthogonal (left singular vectors), the columns of *V* are orthogonal (right singular vectors) and *D* is a diagonal matrix (singular values)


**PCA**

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables


### Components of the SVD - *u* and *v*

In the example below we can see from both the plot in the centre and the plot on the right the singular value decomposition is able to pick up on the change in the mean, both in the columns and in the rows.

```{r}
svd1 <- svd(dataMatrixOrdered)
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, xlab = "Row", ylab = "First left singular vector", pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right seingular vector", pch = 19)
```


### Components of the SVD - variance explained

In the example below we have plotted the raw singular values, but they don't really have very much meaning.

On the right we have the proportion of variance explained - more than half of the variance in the data is explained by a singular value or a single dimension, whereas the remaining variation is explained by the other components.

```{r}
par(mrfow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Proportion of variance explained", pch = 19)
```


### Relationship to principle components

The SVD and the PCA are basically doing the same thing, as the graph below shows - it plots the first principle component on the X axis and the first right singular value on the y axis and they fall exactly in a line

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = "Principal Component 1", ylab = "Right Singular Vector 1")
abline(c(0, 1))
```


### Components of the SVD - variance explained

Below we create a matrix with the first five columns equal to 0 and the second five columns equal to 1. When we calculate the SVD we can see that the first singular value explains 100% of the variation. This shows that even though there are 40 rows and 10 columns in this data set there is really only 1 dimension to it.

```{r}
constantMatrix <- dataMatrixOrdered*2
for(i in 1:dim(dataMatrixOrdered)[1]){constantMatrix[i,] <- rep(c(0, 1), each = 5)}
svd1 <- svd(constantMatrix)
par(mfrow = c(1, 3))
image(t(constantMatrix)[, nrow(constantMatrix):1])
plot(svd1$d, xlab = "Column", ylab = "Singluar value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Colums", ylab = "Proportion of variance explained", pch = 19)
```


### What if we add a second pattern?

Here we add a pattern so that the first half has one mean and the second half has another. There is also a pattern that alternates across the columns

```{r}
set.seed(678910)
for (i in 1:40) {
        # flip a coin
        coinFlip1 <- rbinom(1, size = 1, prob = 0.5)
        coinFlip2 <- rbinom(1, size = 1, prob = 0.5)
        # if coin is heads add a common pattern to that row
        if (coinFlip1) {
                dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0,5), each = 5)
        }
        if (coinFlip2) {
                dataMatrix[i, ] <- dataMatrix[i, ] + rep(c(0, 5), 5)
        }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
```

### Singular value decomposition - true patterns

The purpose of the SVD is to pick up on the patterns buried in the data

```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column", ylab = "Pattern 2")
```


### *v* and patterns of variance in rows

The plot in the middle below somewhat picks up on the block pattern - the first five values are somewhat lower, the second five are somewhat higher. In the panel on the right we can sort of discern the second pattern, as evey other variable is either higher or lower. 

```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")
```


### *d* variance explained

On the panel on the right, the first component explains over 50% of the total variation in the data. This is because the shift pattern (distributed over the first five columns and the second five columns) represents a large amount of variation in the data set.

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained", pch = 19)
```



### Imputing {impute}

You can't run an SVD on a dataset that has missing values. One possibility is to use the impute package. The impute function takes a missing datapoint and imputes its value by the k nearest neighbours to that row.

```{r}
library(impute)  ## Available from http://bioconductor.org
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered))
svd2 <- svd(scale(dataMatrix2))
par(mfrow = c(1, 2))
plot(svd1$v[, 1], pch = 19)
plot(svd2$v[, 1], pch = 19)

```


## Notes and further resources

- Scale matters

- PCs/SVs may mix real patterns

- Can be computationally intensive

